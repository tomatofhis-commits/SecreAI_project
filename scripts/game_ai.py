import subprocess, sys, os, json, threading, chromadb, ctypes, re, time, queue, requests, pygame, psutil
import pygetwindow as gw
from PIL import ImageGrab, ImageTk, Image
import speech_recognition as sr
# import tkinter as tk  # REMOVED: No direct GUI access in this script
import edge_tts
import asyncio
import base64
from io import BytesIO
from datetime import datetime
import io
import contextlib
import atexit
import concurrent.futures
from functools import wraps
import hashlib


# ChromaDBæ¥ç¶šãƒ—ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ¤œç´¢é€Ÿåº¦3-5å€é«˜é€ŸåŒ–ï¼‰
try:
    from .chromadb_pool import get_chroma_collection
except ImportError:
    try:
        from chromadb_pool import get_chroma_collection
    except ImportError:
        get_chroma_collection = None
        print("è­¦å‘Š: chromadb_pool.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ChromaDBæ¥ç¶šãƒ—ãƒ¼ãƒ«ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚")

# APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆAPIã‚³ã‚¹ãƒˆ-40%ã€å¿œç­”é€Ÿåº¦+50%ï¼‰
try:
    from .api_cache_system import APICache
except ImportError:
    try:
        from api_cache_system import APICache
    except ImportError:
        APICache = None
        print("è­¦å‘Š: api_cache_system.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚APIã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚")

# --- 1. ãƒ­ãƒƒã‚¯ã®æº–å‚™ ---
file_lock = threading.Lock()

# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç”¨ã®ä¸¦åˆ—å®Ÿè¡Œé–¢æ•°
def submit_background_task(func, *args, timeout=None):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
    
    Args:
        func: å®Ÿè¡Œã™ã‚‹é–¢æ•°
        *args: é–¢æ•°ã®å¼•æ•°
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰ã€‚Noneã®å ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãªã—
    """
    executor = get_thread_pool_executor()
    
    def wrapped_task():
        try:
            if timeout is not None:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
                run_with_timeout(func, timeout, *args)
            else:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãªã—ã§å®Ÿè¡Œ
                func(*args)
        except Exception as e:
            send_log_to_hub(f"Background Task Error: {e}", is_error=True)
    
    executor.submit(wrapped_task)

# ThreadPoolExecutor for parallel processing (ä¸¦åˆ—å‡¦ç†ç”¨)
_thread_pool_executor = None

def get_thread_pool_executor(max_workers=3):
    """ThreadPoolExecutorã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _thread_pool_executor
    if _thread_pool_executor is None:
        _thread_pool_executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
    return _thread_pool_executor

def run_with_timeout(func, timeout, *args, **kwargs):
    """
    ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
    
    Args:
        func: å®Ÿè¡Œã™ã‚‹é–¢æ•°
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
        *args, **kwargs: é–¢æ•°ã®å¼•æ•°
        
    Returns:
        é–¢æ•°ã®å®Ÿè¡Œçµæœã€‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯None
    """
    executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
    future = executor.submit(func, *args, **kwargs)
    try:
        return future.result(timeout=timeout)
    except concurrent.futures.TimeoutError:
        func_name = getattr(func, '__name__', str(func))
        send_log_to_hub(f"â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {func_name} ({timeout}ç§’)", is_error=True)
        future.cancel()
        return None
    except Exception as e:
        func_name = getattr(func, '__name__', str(func))
        send_log_to_hub(f"âŒ ã‚¨ãƒ©ãƒ¼: {func_name} - {e}", is_error=True)
        return None
    finally:
        executor.shutdown(wait=False)

# ===== pygame.mixerãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ä¿®æ­£ =====
_mixer_initialized = False

def ensure_mixer_cleanup():
    """ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†æ™‚ã«mixerã‚’ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global _mixer_initialized
    if _mixer_initialized and pygame.mixer.get_init():
        try:
            pygame.mixer.music.stop()
            pygame.mixer.quit()
            _mixer_initialized = False
        except:
            pass

atexit.register(ensure_mixer_cleanup)

@contextlib.contextmanager
def managed_mixer(config):
    """pygame.mixerã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ - ä½¿ç”¨å¾Œã«ç¢ºå®Ÿã«ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾"""
    global _mixer_initialized
    target_device = config.get("DEVICE_NAME")
    
    try:
        if not pygame.mixer.get_init():
            try:
                if target_device and target_device != "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ":
                    pygame.mixer.init(frequency=44100, size=-16, channels=1, devicename=target_device)
                else:
                    pygame.mixer.init(frequency=44100, size=-16, channels=1)
                _mixer_initialized = True
            except:
                pygame.mixer.init(frequency=44100, size=-16, channels=1)
                _mixer_initialized = True
        yield
    finally:
        try:
            while pygame.mixer.music.get_busy():
                time.sleep(0.01)
            pygame.mixer.music.unload()
        except:
            pass

# --- 1. ãƒ‘ã‚¹è§£æ±ºãƒ»ãƒ­ã‚°ãƒ»è¨€èªç®¡ç† ---
def get_app_root():
    if getattr(sys, 'frozen', False):
        return os.path.dirname(sys.executable)
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    if os.path.basename(current_script_dir) == "scripts":
        return os.path.dirname(current_script_dir)
    return current_script_dir

def is_voicevox_up():
    """VOICEVOXã‚¨ãƒ³ã‚¸ãƒ³ãŒèµ·å‹•ã—ã¦ãŠã‚Šã€å¿œç­”ã™ã‚‹ã‹ç¢ºèªã™ã‚‹"""
    try:
        response = requests.get("http://127.0.0.1:50021/version", timeout=1)
        return response.status_code == 200
    except:
        return False

APP_ROOT = get_app_root()

def send_log_to_hub(message, is_error=False):
    try:
        url = "http://127.0.0.1:5000/api/log"
        requests.post(url, json={"message": message, "is_error": is_error}, timeout=1)
    except:
        print(message)

def load_lang_file(lang_code):
    path = os.path.join(APP_ROOT, "data", "lang", f"{lang_code}.json")
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {
            "log_messages": {
                "listening": "ã‚·ã‚¹ãƒ†ãƒ : èãå–ã‚Šä¸­...", 
                "module_loaded": "ã‚·ã‚¹ãƒ†ãƒ : è¨˜æ†¶æ›´æ–°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚",
                "engine_starting": "ã‚·ã‚¹ãƒ†ãƒ : éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã‚’èµ·å‹•ä¸­...",
                "engine_fail": "ã‚¨ãƒ©ãƒ¼: ã‚¨ãƒ³ã‚¸ãƒ³èµ·å‹•å¤±æ•— {e}",
                "engine_path_error": "ã‚¨ãƒ©ãƒ¼: ã‚¨ãƒ³ã‚¸ãƒ³ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            },
            "ai_prompt": {
                "role": "ã‚ãªãŸã¯é…ä¿¡ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹é ¼ã‚‚ã—ã„ç›¸æ£’ã§ã™ã€‚", 
                "instruction": "è¦ç‚¹ã‚’ã¾ã¨ã‚ã€ä¸å¯§ãªæ—¥æœ¬èªã§ã€{max_chars}ã€‘ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚", 
                "stt_notice": "", 
                "memory_priority": ""
            },
            "system": {"you_prefix": "You: "}
        }

update_memory = None
try:
    import update_memory
except:
    try:
        from scripts import update_memory
    except:
        update_memory = None

# --- 2. è¨­å®šãƒ»å±¥æ­´ãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç† ---
def load_config_manual(root):
    path = os.path.join(root, "config", "config.json")
    if not os.path.exists(path): return {}, {}, root
    with open(path, "r", encoding="utf-8") as f:
        conf = json.load(f)
    paths = {k: os.path.join(root, v) for k, v in conf.get("FILES", {}).items()}
    return conf, paths, root

def load_history_manual(root):
    path = os.path.join(root, "data", "chat_history.json")
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f: return json.load(f)
        except: return []
    return []

def save_history_manual(history, root):
    path = os.path.join(root, "data", "chat_history.json")
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except: pass

def get_mid_term_context(root):
    path = os.path.join(root, "data", "current_tags.json")
    if not os.path.exists(path): return ""
    try:
        with open(path, "r", encoding="utf-8") as f:
            tags = json.load(f).get("tags", [])
        return f"\nã€å‚è€ƒï¼šéå»ã®è©±é¡Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘\n{', '.join(tags)}\n" if tags else ""
    except: return ""

def get_feedback_context(root):
    path = os.path.join(root, "data", "feedback_memory.json")
    if not os.path.exists(path): return ""
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        pos, neg = data.get("top_positive", []), data.get("top_negative", [])
        ctx = "\n### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è©•ä¾¡ã«åŸºã¥ãè¿½åŠ æŒ‡ç¤º (é‡è¦)\n"
        if pos: ctx += f"ã€æ¯”è¼ƒçš„ã«å¥½è©•ä¾¡ã‚¹ã‚¿ã‚¤ãƒ«ã€‘: {', '.join(pos)}\n"
        if neg: ctx += f"ã€çµ¶å¯¾ã«é¿ã‘ã‚‹ã¹ãä½è©•ä¾¡ã‚¹ã‚¿ã‚¤ãƒ«ã€‘: {', '.join(neg)}\n"
        return ctx
    except: return ""

def search_long_term_memory(query, history=None, root=None, n_results=5):
    """æ”¹å–„ç‰ˆ: æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ãŸé•·æœŸè¨˜æ†¶æ¤œç´¢"""
    try:
        db_path = os.path.join(root, "memory_db")
        if not os.path.exists(db_path): 
            return ""
        
        # æ”¹å–„: æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‹ã‚‰å–å¾—ï¼ˆé«˜é€ŸåŒ–ï¼‰
        if get_chroma_collection:
            collection = get_chroma_collection(db_path)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•
            client_db = chromadb.PersistentClient(path=db_path)
            collection = client_db.get_collection("long_term_memory")
        
        search_query = query
        if history and len(history) >= 1:
            context_snippet = ""
            for msg in history[-2:]:
                clean_msg = re.sub(r"^(ã‚ãªãŸ|AI):\s*", "", msg)
                context_snippet += clean_msg + " "
            search_query = f"{context_snippet.strip()} {query}"
        
        results = collection.query(query_texts=[search_query], n_results=n_results)
        
        if results['documents'] and len(results['documents'][0]) > 0:
            docs = results['documents'][0]
            metas = results['metadatas'][0] if results['metadatas'] else []
            combined = []
            for i in range(len(docs)):
                combined.append({"doc": docs[i], "meta": metas[i] if metas else {}})
            combined.sort(key=lambda x: x["meta"].get("unix") or 0, reverse=True)
            
            context = "\nã€éå»ã®è¨˜æ†¶ã‹ã‚‰ã®é–¢é€£æƒ…å ±ã€‘:\n"
            for item in combined:
                date_val = item["meta"].get("timestamp") or "æ—¥æ™‚ä¸æ˜"
                context += f"ãƒ»[{date_val}] {item['doc']}\n"
            return context
    except Exception as e:
        send_log_to_hub(f"Memory Search Error: {e}", is_error=True)
    return ""

# --- 3. æ¤œç´¢ãƒ»æ·±æ˜ã‚Šå®Ÿè¡Œé–¢æ•° ---
def increment_tavily_count(root):
    """Tavilyã®æ¤œç´¢å›æ•°ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã™ã‚‹ã€‚æœˆãŒå¤‰ã‚ã£ã¦ã„ãŸã‚‰ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã€‚"""
    conf_path = os.path.join(root, "config", "config.json")
    with file_lock:
        try:
            with open(conf_path, "r", encoding="utf-8") as f:
                current_conf = json.load(f)
            
            now = datetime.now() 
            now_month = now.strftime("%Y-%m")
            saved_month = current_conf.get("TAVILY_MONTH", "")
            count = current_conf.get("TAVILY_COUNT", 0)

            if saved_month != now_month:
                count = 1
                current_conf["TAVILY_MONTH"] = now_month
            else:
                count += 1
            
            current_conf["TAVILY_COUNT"] = count
            with open(conf_path, "w", encoding="utf-8") as f:
                json.dump(current_conf, f, indent=4, ensure_ascii=False)
            return count
        except Exception as e:
            send_log_to_hub(f"Count Increment Error: {e}", is_error=True)
            return 0

def execute_background_search(search_query, config, root, session_data):
    summary = None
    try:
        from tavily import TavilyClient
        import ollama
        
        lang_data = load_lang_file(config.get("LANGUAGE", "ja"))
        log_m = lang_data.get("log_messages", {})
        ai_p = lang_data.get("ai_prompt", {})

        count = increment_tavily_count(root)
        
        exec_msg = log_m.get("search_executing", "ã‚·ã‚¹ãƒ†ãƒ : Tavilyæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ (ä»Šæœˆ {count} å›ç›®)").format(count=count)
        send_log_to_hub(exec_msg)

        api_key = config.get("TAVILY_API_KEY")
        summary_model = config.get("MODEL_ID_SUMMARY", "gemma2:9b")
        now = datetime.now()

        # === Tavilyæ¤œç´¢çµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯ ===
        cache_dir = os.path.join(root, "data", "search_cache")
        os.makedirs(cache_dir, exist_ok=True)
        cache_key = hashlib.md5(search_query.encode()).hexdigest()
        cache_file = os.path.join(cache_dir, f"{cache_key}.json")
        cache_ttl_hours = config.get("TAVILY_CACHE_TTL_HOURS", 6)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã—ã€æœ‰åŠ¹æœŸé™å†…ã§ã‚ã‚Œã°ä½¿ç”¨
        if os.path.exists(cache_file):
            cache_age = time.time() - os.path.getmtime(cache_file)
            if cache_age < cache_ttl_hours * 3600:
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cached_data = json.load(f)
                        summary = cached_data['summary']
                        send_log_to_hub("ğŸ’¾ [æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ] éå»ã®æ¤œç´¢çµæœã‚’å†åˆ©ç”¨ï¼ˆTavilyã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰")
                        
                        
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ã‚µãƒãƒªãƒ¼ã§éŸ³å£°å‡ºåŠ›
                        session_id, session_getter, overlay_queue = session_data if session_data else (None, None, None)
                        # å¤ã„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‰ã˜ã¦ã‹ã‚‰æ–°ã—ã„æ¤œç´¢çµæœã‚’è¡¨ç¤º
                        if overlay_queue:
                            overlay_queue.put((None, None, "OFF", 0, 'idle'))
                            time.sleep(0.1)  # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒç¢ºå®Ÿã«é–‰ã˜ã‚‹ã¾ã§å¾…æ©Ÿ
                        
                        prefix = ai_p.get("search_appendix_prefix", "Here is some additional information.")
                        final_text = f"{prefix} {summary}"
                        # skip_idle=False ã§ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’è‡ªå‹•çš„ã«é–‰ã˜ã‚‹
                        speak_and_show(final_text, None, config, root, session_data, show_window=True, skip_idle=False)
                        return
                except Exception as cache_err:
                    send_log_to_hub(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {cache_err}", is_error=True)

        tavily = TavilyClient(api_key=api_key)
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚’å–å¾—
        timeout = config.get("TIMEOUT_WEB_SEARCH", 30)
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§æ¤œç´¢å®Ÿè¡Œ
        def _call_tavily_search():
            return tavily.search(
                query=f"{search_query} info as of {now.strftime('%Y-%m-%d')}", 
                search_depth="advanced", 
                max_results=3
            )
        
        send_log_to_hub(f"ğŸ” Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­... (ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’)")
        search_res = run_with_timeout(_call_tavily_search, timeout)
        
        if search_res is None:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿ
            error_msg = f"â±ï¸ Webæ¤œç´¢ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{timeout}ç§’ï¼‰"
            send_log_to_hub(error_msg, is_error=True)
            return
        
        contents = [f"Source: {r['url']}\nContent: {r['content']}" for r in search_res['results']]
        context = "\n---\n".join(contents)
        
        summary_role = ai_p.get("summary_role", "ã‚ãªãŸã¯å„ªç§€ãªãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®è‹±èªã®æ¤œç´¢çµæœã‚’èª­ã¿ã€å¿…ãšã€æ—¥æœ¬èªã§ã€‘è¦ç‚¹ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚")
        summary_prompt = f"{summary_role}\n\n{context}"
        
        response = ollama.chat(
            model=summary_model, 
            messages=[{'role': 'user', 'content': summary_prompt}]
        )
        summary = response['message']['content']
        
        if summary:
            # === æ¤œç´¢çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ ===
            try:
                cache_data = {
                    'query': search_query,
                    'summary': summary,
                    'timestamp': time.time()
                }
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(cache_data, f, ensure_ascii=False, indent=2)
            except Exception as cache_err:
                send_log_to_hub(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {cache_err}", is_error=True)
            
            # --- ä¿®æ­£ç®‡æ‰€ï¼šä¿å­˜ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œã‚­ãƒ¥ãƒ¼ã«è¿½åŠ  ---
            submit_background_task(save_search_to_db, summary, search_query, config, root)
            
            while pygame.mixer.get_init() and pygame.mixer.music.get_busy():
                time.sleep(0.5)

            # å¤ã„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‰ã˜ã¦ã‹ã‚‰æ–°ã—ã„æ¤œç´¢çµæœã‚’è¡¨ç¤º
            session_id, session_getter, overlay_queue = session_data if session_data else (None, None, None)
            if overlay_queue:
                overlay_queue.put((None, None, "OFF", 0, 'idle'))
                time.sleep(0.1)  # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒç¢ºå®Ÿã«é–‰ã˜ã‚‹ã¾ã§å¾…æ©Ÿ
            
            prefix = ai_p.get("search_appendix_prefix", "Here is some additional information.")
            final_text = f"{prefix} {summary}"
            # skip_idle=False ã§ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’è‡ªå‹•çš„ã«é–‰ã˜ã‚‹
            speak_and_show(final_text, None, config, root, session_data, show_window=True, skip_idle=False)
            
    except Exception as e:
        send_log_to_hub(f"Background Search Error: {e}", is_error=True)

def save_search_to_db(full_summary, query, config, root):
    """æ¤œç´¢çµæœã‚’ã•ã‚‰ã«çŸ­ãè¦ç´„ã—ã¦ç›´æ¥ChromaDBã¸ä¿å­˜ã™ã‚‹ï¼ˆæ”¹å–„ç‰ˆ: æ¥ç¶šãƒ—ãƒ¼ãƒ«ä½¿ç”¨ï¼‰"""
    try:
        import ollama
        import chromadb

        summary_model = config.get("MODEL_ID_SUMMARY", "gemma2:9b")
        
        # 200æ–‡å­—ä»¥å†…ã«ã™ã‚‹ãŸã‚ã®å†è¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        save_prompt = (
            f"ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’ã€å°†æ¥å‚ç…§ã™ã‚‹çŸ¥è­˜ã¨ã—ã¦ã€200æ–‡å­—ä»¥å†…ã®æ—¥æœ¬èªã€‘ã§æ¥µé™ã¾ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n"
            f"å†…å®¹: {full_summary}"
        )
        
        response = ollama.chat(
            model=summary_model,
            messages=[{'role': 'user', 'content': save_prompt}]
        )
        short_summary = response['message']['content'][:200].strip()

        # ChromaDBã¸æ¥ç¶šï¼ˆæ”¹å–„: ãƒ—ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
        db_path = os.path.join(root, "memory_db")
        if get_chroma_collection:
            collection = get_chroma_collection(db_path)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•
            client_db = chromadb.PersistentClient(path=db_path)
            collection = client_db.get_or_create_collection(name="long_term_memory")
        
        now = datetime.now()
        timestamp_str = now.strftime("%Y-%m-%d %H:%M")
        unix_time = time.time()
        
        # æŒ‡å®šã®ã‚¿ã‚°ã¨æ—¥æ™‚ã‚’ä»˜ä¸
        db_content = f"ã€ãƒãƒƒãƒˆæƒ…å ±ã€‘({timestamp_str}) æ¤œç´¢: {query} / å†…å®¹: {short_summary}"
        
        collection.add(
            documents=[db_content],
            metadatas=[{
                "timestamp": timestamp_str, 
                "unix": unix_time,
                "source": "web_search",
                "tag": "ãƒãƒƒãƒˆæƒ…å ±"
            }],
            ids=[f"web_{int(unix_time)}"]
        )
        
        send_log_to_hub(f"ã‚·ã‚¹ãƒ†ãƒ : æ¤œç´¢æƒ…å ±ã‚’ã€Œãƒãƒƒãƒˆæƒ…å ±ã€ã¨ã—ã¦DBã«è¨˜éŒ²ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        send_log_to_hub(f"Internal DB Save Error: {e}", is_error=True)

# --- 4. AIã‚³ã‚¢æ©Ÿèƒ½ ---
gemini_client = None
openai_client = None

def init_ai(config):
    global gemini_client, openai_client
    if config.get("GEMINI_API_KEY"):
        try:
            import google.genai as genai
            gemini_client = genai.Client(api_key=config["GEMINI_API_KEY"])
        except Exception as e:
            send_log_to_hub(f"Gemini Init Error: {e}", is_error=True)
    if config.get("OPENAI_API_KEY"):
        try:
            from openai import OpenAI
            openai_client = OpenAI(api_key=config["OPENAI_API_KEY"])
        except Exception as e:
            send_log_to_hub(f"OpenAI Init Error: {e}", is_error=True)

# APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—
_api_cache_instance = None

def get_api_cache(config):
    """APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _api_cache_instance
    if _api_cache_instance is None and APICache is not None:
        cache_dir = os.path.join(APP_ROOT, "data", "api_cache")
        ttl_hours = config.get("API_CACHE_TTL_HOURS", 24)
        _api_cache_instance = APICache(cache_dir, ttl_hours=ttl_hours)
    return _api_cache_instance

def chat_with_ai(prompt, image=None, config=None, root=None, lang_data=None):
    global gemini_client, openai_client
    history = load_history_manual(root)
    max_chars = config.get("MAX_CHARS", "700æ–‡å­—ä»¥å†…")
    long_term_ctx = search_long_term_memory(prompt, history, root)
    today_ctx_str = f"\nã€ç¾åœ¨ã®çŠ¶æ³ã€‘: {config.get('TODAY_CONTEXT', '')}\n" if config.get('TODAY_CONTEXT') else ""
    feedback_ctx = get_feedback_context(root)
    mid_term_ctx = get_mid_term_context(root)
    p = lang_data["ai_prompt"]
    current_time_str = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    system_instr = (
        f"{p['role']}\n"
        f"{p['instruction'].format(max_chars=max_chars)}\n"
        f"{p['stt_notice']}\n"
        f"{p['memory_priority']}\n"
        f"{today_ctx_str}{long_term_ctx}{feedback_ctx}{mid_term_ctx}"
        f"\nã€å‰ææ¡ä»¶ã«æ—¥æ™‚æƒ…å ±ãŒãªã‘ã‚Œã°ï¼šã€‘ä»Šæ—¥ã¯ {current_time_str} ã§ã™ã€‚"
    )

# æ¤œç´¢ã‚¹ã‚¤ãƒƒãƒãŒã‚ªãƒ³ã®æ™‚ã€è¾æ›¸ã® search_logic ã‚’ä½¿ç”¨
    provider = config.get("AI_PROVIDER", "gemini").lower()
    if config.get("search_switch") is True and provider == "gemini":
        logic = p.get("search_logic", "")
        if logic:
            system_instr += logic
            # ãƒ­ã‚°ã‚’è¿½åŠ 
            send_log_to_hub("ã‚·ã‚¹ãƒ†ãƒ : æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚·ã‚¹ãƒ†ãƒ å‘½ä»¤ã«çµ±åˆã—ã¾ã—ãŸã€‚")
        else:
            send_log_to_hub("è­¦å‘Š: search_switchã¯ONã§ã™ãŒã€ja.jsonå†…ã«search_logicãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", is_error=True)

    answer_text = ""
    image_bytes = None
    image_path_for_cache = None
    
    # APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒã‚§ãƒƒã‚¯ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ãƒ»é«˜é€ŸåŒ–ï¼‰
    cache_enabled = config.get("API_CACHE_ENABLED", True)
    api_cache = get_api_cache(config) if cache_enabled and APICache else None
    model_id = config.get("MODEL_ID", "gemini-2.5-flash")
    
    # ç”»åƒå‡¦ç†
    if image:
        if image.mode != "RGB":
            image = image.convert("RGB")
        buffered = BytesIO()
        image.save(buffered, format="JPEG", quality=95, optimize=True)
        image_bytes = buffered.getvalue()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã«ç”»åƒã‚’ä¸€æ™‚ä¿å­˜
        if api_cache:
            image_path_for_cache = os.path.join(root, "data", "temp_query_image.png")
            os.makedirs(os.path.dirname(image_path_for_cache), exist_ok=True)
            image.save(image_path_for_cache)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã®å–å¾—ã‚’è©¦è¡Œ
    if api_cache:
        cached_response = api_cache.get(prompt, image_path_for_cache, provider, model_id)
        if cached_response:
            send_log_to_hub("ğŸ’¾ [ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ] éå»ã®å¿œç­”ã‚’å†åˆ©ç”¨ï¼ˆAPIã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰")
            # å±¥æ­´ã«è¿½åŠ 
            user_pref = lang_data["system"].get("you_prefix", "You: ")
            history.append(f"{user_pref}{prompt}")
            history.append(f"AI: {cached_response}")
            save_history_manual(history, root)
            return cached_response

    try:
        if provider == "local":
            url = config.get("OLLAMA_URL", "http://localhost:11434/v1")
            model_id = config.get("MODEL_ID_LOCAL", "llama3.2-vision:11b")
            messages = [{"role": "system", "content": system_instr}]
            for h in history[-10:]:
                role = "assistant" if h.startswith("AI:") else "user"
                content = h.replace("AI:", "").replace("You: ", "").replace("ã‚ãªãŸ: ", "").strip()
                messages.append({"role": role, "content": content})
            user_content = [{"type": "text", "text": prompt}]
            if image_bytes:
                base64_image = base64.b64encode(image_bytes).decode('utf-8')
                user_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}})
            messages.append({"role": role, "content": user_content})
            res = requests.post(
                f"{url.rstrip('/')}/chat/completions",
                json={
                    "model": model_id, "messages": messages,
                    "options": {
                        "num_ctx": 4096, "temperature": 0.7, "repeat_penalty": 1.2, 
                        "num_predict": 400, "stop": ["\n\n", "###"]
                    }
                },
                timeout=(10, 600)
            )
            res.raise_for_status()
            answer_text = res.json()['choices'][0]['message']['content']

        elif provider == "openai" and openai_client:
            model_id = config.get("MODEL_ID_GPT", "gpt-5")
            messages = [{"role": "system", "content": system_instr}]
            for h in history[-10:]:
                role = "assistant" if h.startswith("AI:") else "user"
                content = h.replace("AI:", "").replace("You: ", "").replace("ã‚ãªãŸ: ", "").strip()
                messages.append({"role": role, "content": content})
            user_content = [{"type": "text", "text": prompt}]
            if image_bytes:
                base64_image = base64.b64encode(image_bytes).decode('utf-8')
                user_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}})
            messages.append({"role": "user", "content": user_content})
            res = openai_client.chat.completions.create(model=model_id, messages=messages)
            answer_text = res.choices[0].message.content

        elif gemini_client:
            model_id = config.get("MODEL_ID", "gemini-2.5-flash")
            gemini_history = []
            for h in history[-10:]:
                role = "model" if h.startswith("AI:") else "user"
                content = h.replace("AI:", "").replace("You: ", "").replace("ã‚ãªãŸ: ", "").strip()
                gemini_history.append({"role": role, "parts": [{"text": content}]})
            chat = gemini_client.chats.create(model=model_id, config={"system_instruction": system_instr}, history=gemini_history)
            parts = [prompt]
            if image_bytes:
                parts.append(Image.open(BytesIO(image_bytes)))
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã‚’å–å¾—
            timeout = config.get("TIMEOUT_AI_RESPONSE", 60)
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§APIå‘¼ã³å‡ºã—
            def _call_gemini_api():
                return chat.send_message(parts)
            
            send_log_to_hub(f"ğŸ¤– AIå¿œç­”ã‚’å–å¾—ä¸­... (ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’)")
            res = run_with_timeout(_call_gemini_api, timeout)
            
            if res is None:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿ
                error_msg = f"â±ï¸ AIå¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{timeout}ç§’ï¼‰"
                send_log_to_hub(error_msg, is_error=True)
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚AIå¿œç­”ã®å–å¾—ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ããŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
            answer_text = res.text

        if answer_text:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆæ¬¡å›ã®é«˜é€ŸåŒ–ãƒ»ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
            if api_cache:
                try:
                    api_cache.set(prompt, answer_text, image_path_for_cache, provider, model_id)
                except Exception as cache_err:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å¤±æ•—ã¯ç„¡è¦–ï¼ˆæ©Ÿèƒ½ç¶™ç¶šå„ªå…ˆï¼‰
                    pass
            
            user_pref = lang_data["system"].get("you_prefix", "You: ")
            history.append(f"{user_pref}{prompt}")
            history.append(f"AI: {answer_text}")
            save_history_manual(history, root)
            return answer_text

    except Exception as e:
        send_log_to_hub(f"Chat Error ({provider}): {e}", is_error=True)
        return f"AI Error: The conversation stops."

# --- 5. ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤è¡¨ç¤ºãƒ»éŸ³å£°åˆæˆ ---
current_overlay_root = None
speaker_lock = threading.Lock()

# Removed show_window_thread as it is unsafe.
# Logic moved to main_hub.py via overlay_queue.

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯å¯¾ç­–ï¼‰
_mixer_initialized = False

def ensure_mixer_cleanup():
    """ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†æ™‚ã«mixerã‚’ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global _mixer_initialized
    if _mixer_initialized and pygame.mixer.get_init():
        try:
            pygame.mixer.music.stop()
            pygame.mixer.quit()
            _mixer_initialized = False
        except:
            pass

# ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†æ™‚ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ç™»éŒ²
atexit.register(ensure_mixer_cleanup)

@contextlib.contextmanager
def managed_mixer(config):
    """
    pygame.mixerã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    ä½¿ç”¨å¾Œã«ç¢ºå®Ÿã«ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾ã™ã‚‹
    """
    global _mixer_initialized
    target_device = config.get("DEVICE_NAME")
    
    try:
        if not pygame.mixer.get_init():
            try:
                if target_device and target_device != "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ":
                    pygame.mixer.init(frequency=44100, size=-16, channels=1, devicename=target_device)
                else:
                    pygame.mixer.init(frequency=44100, size=-16, channels=1)
                _mixer_initialized = True
            except:
                pygame.mixer.init(frequency=44100, size=-16, channels=1)
                _mixer_initialized = True
        
        yield
        
    finally:
        # å†ç”ŸãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
        try:
            while pygame.mixer.music.get_busy():
                time.sleep(0.01)
            pygame.mixer.music.unload()
        except:
            pass

def speak_and_show(text, image_path=None, config=None, root=None, session_data=None, show_window=True, skip_idle=False):
    if root is None: root = APP_ROOT
    # session_data: (session_id, session_getter, overlay_queue)
    session_id, session_getter, overlay_queue = session_data if session_data else (None, None, None)

    # CHECK SESSION
    if session_id and session_getter:
        if session_getter() != session_id:
            return # Stop processing

    send_log_to_hub(f"AI: {text}")
    lang_code = config.get("LANGUAGE", "ja")
    alpha = config.get("WINDOW_ALPHA", 0.6)
    
    if show_window and str(alpha) != "OFF" and overlay_queue:
        dt = config.get("DISPLAY_TIME", 60)
        # Put request to main thread queue with status 'speaking'
        overlay_queue.put((text, image_path, float(alpha), dt, 'speaking'))

    try:
        if lang_code == "ja":
            # VOICEVOXãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            if is_voicevox_up():
                run_voicevox_speak(text, config, root, session_data)
            else:
                send_log_to_hub("è­¦å‘Š: VOICEVOXã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚edge-ttsã§ä»£ç”¨ã—ã¾ã™ã€‚")
                run_edge_tts_speak(text, "ja", config, root, session_data)
        else:
            run_edge_tts_speak(text, lang_code, config, root, session_data)
    finally:
        # Reset indicator to idle after speech finishes unless explicitly skipped
        if overlay_queue:
            if not skip_idle:
                overlay_queue.put((None, None, "OFF", 0, 'idle'))
            # å°‘ã—å¾…æ©Ÿã—ã¦ã‹ã‚‰ãƒªã‚»ãƒƒãƒˆï¼ˆéŸ³å£°å†ç”Ÿå®Œäº†ã‚’ç¢ºå®Ÿã«ã™ã‚‹ï¼‰
            time.sleep(0.1)

def run_voicevox_speak(text, config, root, session_data):
    """æ”¹å–„ç‰ˆ: ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã‚’å¼·åŒ–ã—ãŸVOICEVOXéŸ³å£°å†ç”Ÿ"""
    session_id, session_getter, _ = session_data if session_data else (None, None, None)
    
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è²¯ã‚ã‚‹ã‚­ãƒ¥ãƒ¼ï¼ˆæœ€å¤§2ã¤åˆ†å…ˆè¡Œç”Ÿæˆã—ã¦ãŠãï¼‰
    audio_queue = queue.Queue(maxsize=2)
    sentences = [s.strip() for s in re.split(r'[ã€‚\nï¼ï¼Ÿ]', text) if s.strip()]
    speaker_id = config.get("SPEAKER_ID", 3)
    speed = config.get("VOICE_SPEED", 1.2)
    
    # --- [å†…éƒ¨é–¢æ•°] éŸ³å£°ã‚’ç”Ÿæˆã—ã¦ã‚­ãƒ¥ãƒ¼ã«å…¥ã‚Œã‚‹ ---
    def generator():
        for s in sentences:
            if session_id and session_getter and session_getter() != session_id: 
                break
            try:
                # 1. ã‚¯ã‚¨ãƒªä½œæˆ
                r1 = requests.post(
                    f"http://127.0.0.1:50021/audio_query?text={s}&speaker={speaker_id}", 
                    timeout=10
                ).json()
                r1["speedScale"] = speed
                r1["volumeScale"] = config.get("VOICE_VOLUME", 1.0)
                r1["postPhonemeLength"] = 0.1
                
                # 2. éŸ³å£°åˆæˆ
                r2 = requests.post(
                    f"http://127.0.0.1:50021/synthesis?speaker={speaker_id}", 
                    data=json.dumps(r1), 
                    timeout=30
                )
                if r2.status_code == 200:
                    audio_queue.put(r2.content)
            except Exception as e:
                send_log_to_hub(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", is_error=True)
        audio_queue.put(None) # çµ‚äº†ã®åˆå›³

    # ç”Ÿæˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
    gen_thread = threading.Thread(target=generator, daemon=True)
    gen_thread.start()

    # --- [å†ç”Ÿãƒ¡ã‚¤ãƒ³å‡¦ç†] - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ãƒªã‚½ãƒ¼ã‚¹ç®¡ç† ---
    with speaker_lock, managed_mixer(config):
        wav_dir = os.path.join(root, "data", "wav")
        os.makedirs(wav_dir, exist_ok=True)

        vol = 1.0  # ã‚¨ãƒ³ã‚¸ãƒ³å´ã§ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã™ã‚‹ãŸã‚å›ºå®š
        
        sentence_idx = 0
        while True:
            audio_data = audio_queue.get()  # ç”ŸæˆãŒçµ‚ã‚ã‚‹ã¾ã§å¾…æ©Ÿ
            if audio_data is None: 
                break  # å…¨æ–‡çµ‚äº†
            
            if session_id and session_getter and session_getter() != session_id:
                pygame.mixer.music.stop()
                break

            # å„æ–‡ã”ã¨ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ã—ã¦Permission deniedã‚’å›é¿
            temp_wav_path = os.path.join(wav_dir, f"current_speech_{sentence_idx}.wav")
            sentence_idx += 1
            
            with open(temp_wav_path, "wb") as f:
                f.write(audio_data)
            
            pygame.mixer.music.load(temp_wav_path)
            pygame.mixer.music.set_volume(vol)
            pygame.mixer.music.play()
            
            while pygame.mixer.music.get_busy():
                if session_id and session_getter and session_getter() != session_id:
                    pygame.mixer.music.stop()
                    break
                time.sleep(0.01)
            
            # å†ç”Ÿå®Œäº†å¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            try:
                pygame.mixer.music.unload()
                os.remove(temp_wav_path)
            except:
                pass

# Edge-TTS è¨€èªã‚³ãƒ¼ãƒ‰ã‹ã‚‰éŸ³å£°åã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
EDGE_TTS_VOICES = {
    "ja": "ja-JP-NanamiNeural",     # æ—¥æœ¬èª
    "en": "en-US-AriaNeural",      # è‹±èªï¼ˆç±³å›½ï¼‰
    "zh": "zh-CN-XiaoxiaoNeural",  # ä¸­å›½èªï¼ˆç°¡ä½“å­—ï¼‰
    "ko": "ko-KR-SunHiNeural",     # éŸ“å›½èª
    "es": "es-ES-ElviraNeural",    # ã‚¹ãƒšã‚¤ãƒ³èª
    "fr": "fr-FR-DeniseNeural",    # ãƒ•ãƒ©ãƒ³ã‚¹èª
    "de": "de-DE-KatjaNeural",     # ãƒ‰ã‚¤ãƒ„èª
    "it": "it-IT-ElsaNeural",      # ã‚¤ã‚¿ãƒªã‚¢èª
    "pt": "pt-BR-FranciscaNeural", # ãƒãƒ«ãƒˆã‚¬ãƒ«èªï¼ˆãƒ–ãƒ©ã‚¸ãƒ«ï¼‰
    "ru": "ru-RU-SvetlanaNeural",  # ãƒ­ã‚·ã‚¢èª
    "vi": "vi-VN-HoaiMyNeural",    # ãƒ™ãƒˆãƒŠãƒ èª
}

def run_edge_tts_speak(text, lang_code, config, root, session_data):
    """æ”¹å–„ç‰ˆ: ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã‚’å¼·åŒ–ã—ãŸEdge-TTSéŸ³å£°å†ç”Ÿ"""
    session_id, session_getter, _ = session_data if session_data else (None, None, None)
    
    voice = EDGE_TTS_VOICES.get(lang_code, "en-US-AriaNeural")
    
    wav_dir = os.path.join(root, "data", "wav")
    os.makedirs(wav_dir, exist_ok=True)
    temp_mp3_path = os.path.join(wav_dir, "edge_tts_speech.mp3")
    
    async def generate_speech():
        """Edge-TTSã§éŸ³å£°ã‚’ç”Ÿæˆ"""
        vol = config.get("VOICE_VOLUME", 1.0)
        perc = int((vol - 1.0) * 100)
        vol_str = f"{perc:+}%"
        communicate = edge_tts.Communicate(text, voice, volume=vol_str)
        await communicate.save(temp_mp3_path)
    
    try:
        # éåŒæœŸã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        asyncio.run(generate_speech())
        
        if session_id and session_getter and session_getter() != session_id:
            return
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
        with managed_mixer(config):
            vol = 1.0  # ã‚¨ãƒ³ã‚¸ãƒ³å´ã§ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã™ã‚‹ãŸã‚å›ºå®š
            pygame.mixer.music.load(temp_mp3_path)
            pygame.mixer.music.set_volume(vol)
            pygame.mixer.music.play()
            
            while pygame.mixer.music.get_busy():
                if session_id and session_getter and session_getter() != session_id:
                    pygame.mixer.music.stop()
                    break
                time.sleep(0.05)

    except Exception as e:
        send_log_to_hub(f"Edge-TTS Playback Error: {e}", is_error=True)
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            if os.path.exists(temp_mp3_path):
                time.sleep(0.1)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯è§£é™¤ã‚’å¾…ã¤
                os.remove(temp_mp3_path)
        except:
            pass

# --- 6. ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ»éŸ³å£°å…¥åŠ›ãƒ»ãƒ¡ã‚¤ãƒ³ ---
def capture_target_screenshot(config, root):
    target = config.get("TARGET_GAME_TITLE", "All Capture")
    path = os.path.join(root, "data", "temp_ss.png")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    try:
        if target == "All Capture": raise ValueError()
        wins = gw.getWindowsWithTitle(target)
        win = next((w for w in wins if w.title == target), None)
        if win:
            bbox = (win.left, win.top, win.right, win.bottom)
            ImageGrab.grab(bbox=bbox, all_screens=True).save(path)
        else: ImageGrab.grab(all_screens=True).save(path)
    except: ImageGrab.grab(all_screens=True).save(path)
    return path

def ensure_voicevox_is_running(config, lang_data):
    vv_path = config.get("VV_PATH", "")
    target_name = os.path.basename(vv_path)
    if not target_name: return False
    for proc in psutil.process_iter(['name']):
        try:
            if target_name.lower() in proc.info['name'].lower(): return True
        except: continue
    if vv_path and os.path.exists(vv_path):
        send_log_to_hub(lang_data["log_messages"]["engine_starting"])
        try:
            subprocess.Popen([vv_path], creationflags=subprocess.CREATE_NO_WINDOW | subprocess.DETACHED_PROCESS)
            time.sleep(3); return True
        except Exception as e:
            send_log_to_hub(lang_data["log_messages"]["engine_fail"].format(e=e), is_error=True)
    else: send_log_to_hub(lang_data["log_messages"]["engine_path_error"], is_error=True)
    return False

def get_voice_input(guide, config, root, lang_data, session_data, image_path=None):
    stt_lang = 'ja-JP' if config.get("LANGUAGE", "ja") == "ja" else 'en-US'
    session_id, session_getter, _ = session_data if session_data else (None, None, None)

    # ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹éŸ³å£°ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å†ç”Ÿï¼ˆåŒæ™‚ã«å…¥åŠ›ã‚’é–‹å§‹ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰
    threading.Thread(
        target=speak_and_show, 
        args=(guide, image_path, config, root, session_data, False, True), 
        daemon=True
    ).start()
    
    # å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ï¼ˆãƒã‚¤ã‚¯ï¼‰ã®è¨­å®šã‚’è§£æ±º
    input_device_name = config.get("INPUT_DEVICE_NAME", "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ")
    device_index = None
    if input_device_name and input_device_name != "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ":
        try:
            mic_list = sr.Microphone.list_microphone_names()
            if input_device_name in mic_list:
                device_index = mic_list.index(input_device_name)
        except: pass

    r = sr.Recognizer()
    # çµ‚äº†ã‚’åˆ¤æ–­ã™ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼ˆæ²ˆé»™ã‚’è¨±å®¹ã™ã‚‹æ™‚é–“ï¼‰ã‚’å°‘ã—é•·ãã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.8ç§’ï¼‰
    r.pause_threshold = 1.2
    
    try:
        with sr.Microphone(device_index=device_index) as source:
            r.adjust_for_ambient_noise(source, duration=0.8)
            send_log_to_hub(lang_data["log_messages"]["listening"])
            
            # Send status:listening to overlay
            if session_data and session_data[2]:
                session_data[2].put(("", None, "OFF", 0, 'listening'))
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
            if session_id and session_getter and session_getter() != session_id: return None
            
            # å¾…æ©Ÿæ™‚é–“ã‚’10ç§’ã€ç™ºè©±åˆ¶é™ã‚’20ç§’ã«å»¶é•·
            audio = r.listen(source, timeout=10, phrase_time_limit=20)
            
            if session_id and session_getter and session_getter() != session_id: return None
            return r.recognize_google(audio, language=stt_lang)
    except: return None

def main(mode="voice", chat_text=None, session_id=None, session_getter=None, overlay_queue=None):
    session_data = (session_id, session_getter, overlay_queue)
    root = get_app_root()
    config, _, _ = load_config_manual(root)
    lang_code = config.get("LANGUAGE", "ja")
    lang_data = load_lang_file(lang_code)
    log_m = lang_data.get("log_messages", {})

    if lang_code == "ja": ensure_voicevox_is_running(config, lang_data)
    init_ai(config)
    init_ai(config)
    # Stop flag removal logic is no longer needed
    # stop_flag = os.path.join(root, "stop.flag")
    # if os.path.exists(stop_flag):
    #     try: os.remove(stop_flag)
    #     except: pass

    try:
        query, abs_path = None, None
        if mode == "vision":
            abs_path = os.path.abspath(capture_target_screenshot(config, root))
            query = get_voice_input(log_m.get("vision_guide", "Analyze"), config, root, lang_data, session_data, abs_path)
        elif mode == "chat" and chat_text:
            query = chat_text
        else:
            query = get_voice_input(log_m.get("voice_guide", "How can I help you?"), config, root, lang_data, session_data)

        if query:
            # Send status:thinking to overlay
            if session_data and session_data[2]:
                session_data[2].put(("", None, "OFF", 0, 'thinking'))
                
            # --- ãƒ¢ãƒ¼ãƒ‰åˆ†å²ï¼šè¤‡åˆAIãƒ¢ãƒ¼ãƒ‰ã‹é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã‹ ---
            res = None
            if config.get("USE_INTERSECTING_AI", False):
                try:
                    from scripts.intersecting_ai import run_intersecting_ai
                    send_log_to_hub("ã‚·ã‚¹ãƒ†ãƒ : è¤‡åˆAIãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
                    
                    # ä¿®æ­£å‰: res = run_intersecting_ai(query, abs_path, root)
                    # ä¿®æ­£å¾Œ: å®šç¾©ã«åˆã‚ã›ã¦ 5ã¤ã®å¼•æ•° ã™ã¹ã¦ã‚’æ¸¡ã—ã¾ã™
                    res = run_intersecting_ai(query, abs_path, config, root, lang_data)
                    
                except Exception as e:
                    send_log_to_hub(f"è¤‡åˆAIã‚¨ãƒ©ãƒ¼: {e}ã€‚é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚", is_error=True)
            
            # è¤‡åˆAIãŒã‚ªãƒ•ã€ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ã§ res ãŒç©ºã®å ´åˆã«é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
            if not res:
                res = chat_with_ai(query, Image.open(abs_path) if abs_path else None, config, root, lang_data)

            if res:
                search_match = re.search(r'\[SEARCH:\s*(.*?)\]', res)
                clean_res = re.sub(r'\[SEARCH:.*\]', '', res).strip()

                if search_match and config.get("search_switch") is True and config.get("AI_PROVIDER", "gemini").lower() == "gemini":
                    s_query = search_match.group(1)
                    # è¾æ›¸ã‹ã‚‰äºˆç´„ãƒ­ã‚°ã‚’å–å¾—
                    res_msg = log_m.get("search_reserved", "ã‚·ã‚¹ãƒ†ãƒ : æ¤œç´¢ã‚¿ã‚¹ã‚¯ã‚’äºˆç´„ã—ã¾ã—ãŸã€‚")
                    send_log_to_hub(res_msg)
                    # Use session_data instead of stop_flag in background tasks if possible, 
                    # but for now we pass session_data as the last arg to execute_background_search 
                    # replacing stop_flag. Logic inside execute_background_search needs update for this.
                    # æ¤œç´¢ã‚¿ã‚¹ã‚¯ã¯éŸ³å£°èª­ã¿ä¸Šã’ã‚’å«ã‚€ãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãªã—ã§å®Ÿè¡Œ
                    submit_background_task(execute_background_search, s_query, config, root, session_data, timeout=None)

                speak_and_show(clean_res, abs_path, config, root, session_data)

        if update_memory and len(load_history_manual(root)) >= 16:
            # è¾æ›¸ã‹ã‚‰äºˆç´„ãƒ­ã‚°ã‚’å–å¾—
            mem_msg = log_m.get("memory_update_reserved", "ã‚·ã‚¹ãƒ†ãƒ : è¨˜æ†¶æ•´ç†ã‚¿ã‚¹ã‚¯ã‚’äºˆç´„ã—ã¾ã—ãŸã€‚")
            send_log_to_hub(mem_msg)
            # ãƒ¡ãƒ¢ãƒªæ›´æ–°ã‚¿ã‚¹ã‚¯ã¯120ç§’ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§å®Ÿè¡Œ
            submit_background_task(update_memory.main, root, timeout=120)
            
        # global current_overlay_root  # Removed
        # if current_overlay_root:
        #     try: current_overlay_root.after(0, current_overlay_root.destroy)
        #     except: pass
    except Exception as e:
        send_log_to_hub(f"Execution Error: {e}", is_error=True)

if __name__ == "__main__":
    m = sys.argv[1] if len(sys.argv) > 1 else "voice"
    t = sys.argv[2] if len(sys.argv) > 2 else None
    main(m, t)